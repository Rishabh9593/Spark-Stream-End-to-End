scala> rishabh@Amazon:~/spark/spark-3.2.1-bin-hadoop3.2/bin$ ./spark-shell --packages "com.datastax.spark:spark-cassandra-connector_2.11:2.0.2","org.apache.spark:spark-streaming-kafka-0-8_2.11:2.0.0"
22/06/14 18:55:40 WARN Utils: Your hostname, Amazon resolves to a loopback address: 127.0.1.1; using 192.168.1.8 instead (on interface wlp2s0)
22/06/14 18:55:40 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
WARNING: An illegal reflective access operation has occurred
WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/rishabh/spark/spark-3.2.1-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)
WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
WARNING: All illegal access operations will be denied in a future release
:: loading settings :: url = jar:file:/home/rishabh/spark/spark-3.2.1-bin-hadoop3.2/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
Ivy Default Cache set to: /home/rishabh/.ivy2/cache
The jars for the packages stored in: /home/rishabh/.ivy2/jars
com.datastax.spark#spark-cassandra-connector_2.11 added as a dependency
org.apache.spark#spark-streaming-kafka-0-8_2.11 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-f45b3aa4-fe20-4840-acaf-53ae6f260d9c;1.0
	confs: [default]
	found com.datastax.spark#spark-cassandra-connector_2.11;2.0.2 in central
	found com.twitter#jsr166e;1.1.0 in central
	found commons-beanutils#commons-beanutils;1.9.3 in central
	found commons-collections#commons-collections;3.2.2 in central
	found org.joda#joda-convert;1.2 in central
	found joda-time#joda-time;2.3 in central
	found io.netty#netty-all;4.0.33.Final in central
	found org.scala-lang#scala-reflect;2.11.8 in central
	found org.apache.spark#spark-streaming-kafka-0-8_2.11;2.0.0 in central
	found org.apache.kafka#kafka_2.11;0.8.2.1 in central
	found org.scala-lang.modules#scala-xml_2.11;1.0.2 in central
	found com.yammer.metrics#metrics-core;2.2.0 in central
	found org.slf4j#slf4j-api;1.7.16 in central
	found org.scala-lang.modules#scala-parser-combinators_2.11;1.0.2 in central
	found com.101tec#zkclient;0.3 in central
	found log4j#log4j;1.2.17 in central
	found org.apache.kafka#kafka-clients;0.8.2.1 in central
	found net.jpountz.lz4#lz4;1.3.0 in central
	found org.xerial.snappy#snappy-java;1.1.2.4 in central
	found org.spark-project.spark#unused;1.0.0 in central
:: resolution report :: resolve 692ms :: artifacts dl 19ms
	:: modules in use:
	com.101tec#zkclient;0.3 from central in [default]
	com.datastax.spark#spark-cassandra-connector_2.11;2.0.2 from central in [default]
	com.twitter#jsr166e;1.1.0 from central in [default]
	com.yammer.metrics#metrics-core;2.2.0 from central in [default]
	commons-beanutils#commons-beanutils;1.9.3 from central in [default]
	commons-collections#commons-collections;3.2.2 from central in [default]
	io.netty#netty-all;4.0.33.Final from central in [default]
	joda-time#joda-time;2.3 from central in [default]
	log4j#log4j;1.2.17 from central in [default]
	net.jpountz.lz4#lz4;1.3.0 from central in [default]
	org.apache.kafka#kafka-clients;0.8.2.1 from central in [default]
	org.apache.kafka#kafka_2.11;0.8.2.1 from central in [default]
	org.apache.spark#spark-streaming-kafka-0-8_2.11;2.0.0 from central in [default]
	org.joda#joda-convert;1.2 from central in [default]
	org.scala-lang#scala-reflect;2.11.8 from central in [default]
	org.scala-lang.modules#scala-parser-combinators_2.11;1.0.2 from central in [default]
	org.scala-lang.modules#scala-xml_2.11;1.0.2 from central in [default]
	org.slf4j#slf4j-api;1.7.16 from central in [default]
	org.spark-project.spark#unused;1.0.0 from central in [default]
	org.xerial.snappy#snappy-java;1.1.2.4 from central in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   20  |   0   |   0   |   0   ||   20  |   0   |
	---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent-f45b3aa4-fe20-4840-acaf-53ae6f260d9c
	confs: [default]
	0 artifacts copied, 20 already retrieved (0kB/15ms)
22/06/14 18:55:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
Spark context Web UI available at http://192.168.1.8:4040
Spark context available as 'sc' (master = local[*], app id = local-1655213147972).
Spark session available as 'spark'.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 3.2.1
      /_/
         
Using Scala version 2.12.15 (OpenJDK 64-Bit Server VM, Java 11.0.15)
Type in expressions to have them evaluated.
Type :help for more information.


scala> import org.apache.spark.SparkConf
import org.apache.spark.SparkConf

scala> 

scala> import org.apache.spark.SparkContext._
import org.apache.spark.SparkContext._

scala> 

scala> import org.apache.spark.streaming._
import org.apache.spark.streaming._

scala> 

scala> import org.apache.spark.streaming.StreamingContext._
import org.apache.spark.streaming.StreamingContext._

scala> 

scala> import org.apache.spark.streaming.kafka._
import org.apache.spark.streaming.kafka._

scala> 

scala> 

scala> 

scala> import com.datastax.spark.connector.SomeColumns
import com.datastax.spark.connector.SomeColumns

scala> 

scala> import com.datastax.spark.connector.cql.CassandraConnector
import com.datastax.spark.connector.cql.CassandraConnector

scala> 

scala> import com.datastax.spark.connector.streaming._
import com.datastax.spark.connector.streaming._

scala> val sparkConf = new SparkConf().setAppName("KafkaSparkStreaming").set("spark.cassandra.connection.host", "127.0.0.1")
sparkConf: org.apache.spark.SparkConf = org.apache.spark.SparkConf@907a731

scala> val ssc = new StreamingContext(sparkConf, Seconds(20))
org.apache.spark.SparkException: Only one SparkContext should be running in this JVM (see SPARK-2243).The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:943)
org.apache.spark.repl.Main$.createSparkSession(Main.scala:106)
<init>(<console>:15)
<init>(<console>:42)
<init>(<console>:44)
.<init>(<console>:48)
.<clinit>(<console>)
.lzycompute(<console>:7)
.$print(<console>:6)
$print(<console>)
java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.base/java.lang.reflect.Method.invoke(Method.java:566)
scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:747)
scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1020)
scala.tools.nsc.interpreter.IMain.$anonfun$interpret$1(IMain.scala:568)
scala.reflect.internal.util.ScalaClassLoader.asContext(ScalaClassLoader.scala:36)
scala.reflect.internal.util.ScalaClassLoader.asContext$(ScalaClassLoader.scala:116)
scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:41)
  at org.apache.spark.SparkContext$.$anonfun$assertNoOtherContextIsRunning$2(SparkContext.scala:2647)
  at scala.Option.foreach(Option.scala:407)
  at org.apache.spark.SparkContext$.assertNoOtherContextIsRunning(SparkContext.scala:2644)
  at org.apache.spark.SparkContext$.markPartiallyConstructed(SparkContext.scala:2734)
  at org.apache.spark.SparkContext.<init>(SparkContext.scala:95)
  at org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:851)
  at org.apache.spark.streaming.StreamingContext.<init>(StreamingContext.scala:85)
  ... 57 elided

scala> spark.stop

scala> val ssc = new StreamingContext(sparkConf, Seconds(20))
ssc: org.apache.spark.streaming.StreamingContext = org.apache.spark.streaming.StreamingContext@51a6748f

scala> val topicpMap = "mytopic".split(",").map((_, 1.toInt)).toMap
topicpMap: scala.collection.immutable.Map[String,Int] = Map(mytopic -> 1)

scala> 

scala> val lines = KafkaUtils.createStream(ssc, "localhost:2181", "sparkgroup", topicpMap).map(_._2)
lines: org.apache.spark.streaming.dstream.DStream[String] = org.apache.spark.streaming.dstream.MappedDStream@67d5c07

scala> 

scala> lines.print();

scala> 

scala> 

scala> 

scala> lines.map(line => { val arr = line.split(","); (arr(0),arr(1),arr(2),arr(3),arr(4)) }).saveToCassandra("sparkdata", "cust_data", SomeColumns("fname", "lname","url","product","cnt"))

scala> 

scala> ssc.start
Exception in thread "streaming-start" java.lang.NoClassDefFoundError: org/apache/spark/internal/Logging$class
	at org.apache.spark.streaming.kafka.KafkaReceiver.<init>(KafkaInputDStream.scala:76)
	at org.apache.spark.streaming.kafka.KafkaInputDStream.getReceiver(KafkaInputDStream.scala:60)
	at org.apache.spark.streaming.scheduler.ReceiverTracker.$anonfun$launchReceivers$1(ReceiverTracker.scala:438)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)
	at org.apache.spark.streaming.scheduler.ReceiverTracker.launchReceivers(ReceiverTracker.scala:437)
	at org.apache.spark.streaming.scheduler.ReceiverTracker.start(ReceiverTracker.scala:159)
	at org.apache.spark.streaming.scheduler.JobScheduler.start(JobScheduler.scala:101)
	at org.apache.spark.streaming.StreamingContext.$anonfun$start$1(StreamingContext.scala:590)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.ThreadUtils$$anon$2.run(ThreadUtils.scala:218)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.internal.Logging$class
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:589)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522)
	... 16 more


